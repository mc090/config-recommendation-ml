{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b43290f",
   "metadata": {},
   "source": [
    "# Dataset exploration\n",
    "This notebook runs the project pipeline steps in order:\n",
    "1. fetch raw data\n",
    "2. extract structure\n",
    "3. compute features\n",
    "4. build dataset. \n",
    "\n",
    "It shows mocked process of dataset extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4642eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Notebook] Configs loaded:\n",
      "{'raw_dir': 'data/raw/', 'interim_dir': 'data/interim/', 'processed_dir': 'data/processed/', 'dataset_filename': 'dataset.csv', 'overwrite': False}\n",
      "{'data_seed': 90, 'sampling_seed': 90, 'training_seed': 90, 'evaluation_seed': 90, 'deterministic': {'force_hash_sorting': True, 'enforce_timestamp_free_ops': True}}\n",
      "[Notebook] Fetching raw data...\n",
      "[fetch_raw] Saved mock raw metadata to data/raw/raw_metadata.json\n",
      "[Notebook] Extracting structure...\n",
      "[extract_structure] Saved mock structure to data/interim/structure.json\n",
      "[Notebook] Computing features...\n",
      "[compute_features] Saved mock features to data/interim/features.json\n",
      "[Notebook] Building dataset...\n",
      "[build_dataset] Mock dataset saved to data/processed/dataset.csv\n",
      "[Notebook] Pipeline completed (or attempted).\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from src.data.build_dataset import build_dataset\n",
    "from src.data.compute_features import compute_features\n",
    "from src.data.extract_structure import extract_structure\n",
    "from src.data.fetch_raw import fetch_raw\n",
    "from src.utils.config import load_config\n",
    "\n",
    "data_cfg = load_config(\"data\")\n",
    "seeds_cfg = load_config(\"seeds\")\n",
    "\n",
    "print(\"[Notebook] Configs loaded:\")\n",
    "print(data_cfg)\n",
    "print(seeds_cfg)\n",
    "\n",
    "print(\"[Notebook] Fetching raw data...\")\n",
    "fetch_raw()\n",
    "\n",
    "print(\"[Notebook] Extracting structure...\")\n",
    "extract_structure()\n",
    "\n",
    "print(\"[Notebook] Computing features...\")\n",
    "compute_features()\n",
    "\n",
    "print(\"[Notebook] Building dataset...\")\n",
    "build_dataset()\n",
    "\n",
    "print(\"[Notebook] Pipeline completed (or attempted).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141b875b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Notebook] Dataset exists at data/processed/dataset.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_url</th>\n",
       "      <th>num_py_files</th>\n",
       "      <th>num_js_files</th>\n",
       "      <th>num_notebooks</th>\n",
       "      <th>avg_files_per_dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/user/repo1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/user/repo2</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        repo_url  num_py_files  num_js_files  num_notebooks  \\\n",
       "0  https://github.com/user/repo1             4             5              0   \n",
       "1  https://github.com/user/repo2             8             5              2   \n",
       "\n",
       "   avg_files_per_dir  \n",
       "0                  2  \n",
       "1                  1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       num_py_files  num_js_files  num_notebooks  avg_files_per_dir\n",
      "count      2.000000           2.0       2.000000           2.000000\n",
      "mean       6.000000           5.0       1.000000           1.500000\n",
      "std        2.828427           0.0       1.414214           0.707107\n",
      "min        4.000000           5.0       0.000000           1.000000\n",
      "25%        5.000000           5.0       0.500000           1.250000\n",
      "50%        6.000000           5.0       1.000000           1.500000\n",
      "75%        7.000000           5.0       1.500000           1.750000\n",
      "max        8.000000           5.0       2.000000           2.000000\n"
     ]
    }
   ],
   "source": [
    "dataset_file = Path(data_cfg.get(\"processed_dir\", \"data/processed\")) / data_cfg.get(\n",
    "    \"dataset_filename\", \"dataset.csv\"\n",
    ")\n",
    "dataset_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not dataset_file.exists():\n",
    "    print(\n",
    "        \"[Notebook] Dataset not found after pipeline. Check previous cells for errors.\"\n",
    "    )\n",
    "else:\n",
    "    print(f\"[Notebook] Dataset exists at {dataset_file}\")\n",
    "\n",
    "    df = pd.read_csv(dataset_file)\n",
    "    display(df.head())\n",
    "    print(df.describe())\n",
    "\n",
    "    if \"stars\" in df.columns:\n",
    "        df[\"stars_scaled\"] = df[\"stars\"] / df[\"stars\"].max()\n",
    "        df.hist(column=\"stars_scaled\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
